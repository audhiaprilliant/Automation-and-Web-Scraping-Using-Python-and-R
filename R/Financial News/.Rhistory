str(volume_news)
# SAVE THE RESULT
write.csv(x = volume_news,
file = 'file:///D:/Audhi Aprilliant/IPB Works/Statistics Department/My Undergraduate Thesis/You Can Do It, Audhi/4 Datasets/6 Forecasting Data/3 Daily of Volume News/3 Daily Volume News 2019-01-01 and 2019-12-30.csv',
row.names = FALSE)
library(tm)
# Import external function
source(file = 'D:/Audhi Aprilliant/IPB Works/Statistics Department/My Undergraduate Thesis/You Can Do It, Audhi/5 R-Scripts/Helpers/Function Helper/Cleansing.R')
source(file = 'D:/Audhi Aprilliant/IPB Works/Statistics Department/My Undergraduate Thesis/You Can Do It, Audhi/5 R-Scripts/Helpers/Function Helper/Lexicon-Based Scoring Analysis.R')
# LOAD DATA
news_data = read.csv(file = 'file:///D:/Audhi Aprilliant/IPB Works/Statistics Department/My Undergraduate Thesis/You Can Do It, Audhi/4 Datasets/2 News Data/Okezone Data - Financial News/Clean Title - Financial Okezone 2019-01-01 and 2019-12-30.csv',
header = TRUE,
sep = ','
#,stringsAsFactors = FALSE,
#fileEncoding = "latin1"
)
View(news_data)
dim(news_data)
news_data_txt = news_data$title
length(news_data_txt)
# WORK WITH CORPUS
news_data_corpus = VCorpus(VectorSource(news_data_txt))
inspect(news_data_corpus[[5]])
# 1 Tokenization - case folding and filtering
news_data_corpus = tm_map(news_data_corpus, content_transformer(tolower))
news_data_corpus = tm_map(news_data_corpus, toSpace, "[[:punct:]]") # punctuation
news_data_corpus = tm_map(news_data_corpus, toSpace, "[[:digit:]]") # numbers
news_data_corpus = tm_map(news_data_corpus, stripWhitespace)        # Eliminate extra white spaces
# 2 First scoring
pos = readLines('D:/Audhi Aprilliant/IPB Works/Statistics Department/My Undergraduate Thesis/You Can Do It, Audhi/5 R-Scripts/Helpers/Data Helper/s-pos.txt')
neg = readLines('D:/Audhi Aprilliant/IPB Works/Statistics Department/My Undergraduate Thesis/You Can Do It, Audhi/5 R-Scripts/Helpers/Data Helper/s-neg.txt')
df_news = data.frame(text = sapply(news_data_corpus, as.character),
stringsAsFactors = FALSE)
# Negation Handling
negasi = scan('D:/Audhi Aprilliant/IPB Works/Statistics Department/My Undergraduate Thesis/You Can Do It, Audhi/5 R-Scripts/Helpers/Data Helper/negatingword.txt',
what = 'character')
senti = read.csv('file:///D:/Audhi Aprilliant/IPB Works/Statistics Department/My Undergraduate Thesis/You Can Do It, Audhi/5 R-Scripts/Helpers/Data Helper/sentiwords_id_compiled.txt',
sep = ':',
header = FALSE) %>%
mutate(words = as.character(V1),
score = as.numeric(V2)) %>%
select(c('words','score'))
booster = read.csv('D:/Audhi Aprilliant/IPB Works/Statistics Department/My Undergraduate Thesis/You Can Do It, Audhi/5 R-Scripts/Helpers/Data Helper/boosterwords_id.txt',
sep = ":",
header = FALSE) %>%
mutate(words = as.character(V1),
score = as.numeric(V2)) %>%
select(c('words','score'))
results_1 = scores.sentiment(df_news$text, senti, negasi)
# 2 Second scoring
# Stemming
news_data_corpus = tm_map(news_data_corpus, content_transformer(stemming))
df_news = data.frame(text = sapply(news_data_corpus, as.character),
stringsAsFactors = FALSE)
results_2 = scores.sentiment(df_news$text, senti, negasi)
# 3 Check Inconsistency from lexicon-based approachs
check_data = data.frame(results_1,
results_2)
check_data$final = check_data$score + check_data$score.1
# 4 Convert score to sentiment classes
check_data$class = as.factor(ifelse(check_data$final < 0, 'Negative',
ifelse(check_data$final == 0, 'Neutral','Positive')))
save_data = data.frame(news_data,
class = check_data$class)
write.csv(x = save_data,
file = 'file:///D:/Audhi Aprilliant/IPB Works/Statistics Department/My Undergraduate Thesis/You Can Do It, Audhi/4 Datasets/6 Forecasting Data/0 News with Sentiment/7 Kompas  2019-01-01 and 2019-12-30 News with Class.csv',
row.names = FALSE,
quote = FALSE)
# 5 Generating daily percentage of sentiment
news_data_senti = data.frame(news_data,
class = check_data$class)
unique(news_data_senti$class)
date_data = as.character(unique(news_data_senti$date))
number_pos = rep(x = 0,length.out = length(date_data))
number_neg = rep(x = 0,length.out = length(date_data))
# Count number of daily positive sentiment
for (i in 1:length(date_data)) {
for (j in 1:dim(news_data_senti)[1]) {
if (date_data[i] == news_data_senti$date[j] & news_data_senti$class[j] == 'Positive') {
number_pos[i] = number_pos[i] + 1
}
else {
number_pos[i] = number_pos[i] + 0
}
}
}
# Count number of daily negative sentiment
for (i in 1:length(date_data)) {
for (j in 1:dim(news_data_senti)[1]) {
if (date_data[i] == news_data_senti$date[j] & news_data_senti$class[j] == 'Negative') {
number_neg[i] = number_neg[i] + 1
}
else {
number_neg[i] = number_neg[i] + 0
}
}
}
# Compiled data
news_data_senti_daily = data.frame(date = date_data,
num_pos = number_pos,
num_neg = number_neg)
news_data_senti_daily
write.csv(x = news_data_senti_daily,
file = 'file:///D:/Audhi Aprilliant/IPB Works/Statistics Department/My Undergraduate Thesis/You Can Do It, Audhi/4 Datasets/6 Forecasting Data/1 Daily of Sentiment Percentage/7 Okezone 2019-01-01 and 2019-12-30 Daily Percentage of Sentiment.csv',
row.names = FALSE)
# 6 Stopwords removal
rm_stopword = VCorpus(VectorSource(results_2$text))
# Using edited stopword list
stopwords_id = readLines('D:/Audhi Aprilliant/IPB Works/Statistics Department/My Undergraduate Thesis/You Can Do It, Audhi/5 R-Scripts/Helpers/Data Helper/stopwords-id.txt')
stopwords_id
rm_stopword = tm_map(rm_stopword, removeWords, stopwords_id)
# Check the final result
inspect(rm_stopword[[5]])
# Eliminate extra white spaces
rm_stopword = tm_map(rm_stopword, stripWhitespace)
# Check the final result
inspect(rm_stopword[[5]])
# Save the data
clean_data = data.frame(text = sapply(rm_stopword, as.character),
stringsAsFactors = FALSE)
clean_data = data.frame(date = news_data$date,
title = clean_data,
time = news_data$time)
# Save the data
write.csv(x = clean_data,
file = 'file:///D:/Audhi Aprilliant/IPB Works/Statistics Department/My Undergraduate Thesis/You Can Do It, Audhi/4 Datasets/6 Forecasting Data/2 Clean Headline News for TF-IDF/7 Okezone 2019-01-01 and 2019-12-30 Clean Title Stopwords Removal.csv',
row.names = FALSE)
# LOAD DATA
news_data = read.csv(file = 'file:///D:/Audhi Aprilliant/IPB Works/Statistics Department/My Undergraduate Thesis/You Can Do It, Audhi/4 Datasets/2 News Data/Kompas Data - Financial News/Clean Title - Financial Kompas 2019-01-01 and 2019-12-30.csv',
header = TRUE,
sep = ','
#,stringsAsFactors = FALSE,
#fileEncoding = "latin1"
)
dim(news_data)
news_data_txt = news_data$title
length(news_data_txt)
# WORK WITH CORPUS
news_data_corpus = VCorpus(VectorSource(news_data_txt))
inspect(news_data_corpus[[5]])
# 1 Tokenization - case folding and filtering
news_data_corpus = tm_map(news_data_corpus, content_transformer(tolower))
news_data_corpus = tm_map(news_data_corpus, toSpace, "[[:punct:]]") # punctuation
news_data_corpus = tm_map(news_data_corpus, toSpace, "[[:digit:]]") # numbers
news_data_corpus = tm_map(news_data_corpus, stripWhitespace)        # Eliminate extra white spaces
pos = readLines('D:/Audhi Aprilliant/IPB Works/Statistics Department/My Undergraduate Thesis/You Can Do It, Audhi/5 R-Scripts/Helpers/Data Helper/s-pos.txt')
neg = readLines('D:/Audhi Aprilliant/IPB Works/Statistics Department/My Undergraduate Thesis/You Can Do It, Audhi/5 R-Scripts/Helpers/Data Helper/s-neg.txt')
df_news = data.frame(text = sapply(news_data_corpus, as.character),
stringsAsFactors = FALSE)
# Negation Handling
negasi = scan('D:/Audhi Aprilliant/IPB Works/Statistics Department/My Undergraduate Thesis/You Can Do It, Audhi/5 R-Scripts/Helpers/Data Helper/negatingword.txt',
what = 'character')
senti = read.csv('file:///D:/Audhi Aprilliant/IPB Works/Statistics Department/My Undergraduate Thesis/You Can Do It, Audhi/5 R-Scripts/Helpers/Data Helper/sentiwords_id_compiled.txt',
sep = ':',
header = FALSE) %>%
mutate(words = as.character(V1),
score = as.numeric(V2)) %>%
select(c('words','score'))
booster = read.csv('D:/Audhi Aprilliant/IPB Works/Statistics Department/My Undergraduate Thesis/You Can Do It, Audhi/5 R-Scripts/Helpers/Data Helper/boosterwords_id.txt',
sep = ":",
header = FALSE) %>%
mutate(words = as.character(V1),
score = as.numeric(V2)) %>%
select(c('words','score'))
results_1 = scores.sentiment(df_news$text, senti, negasi)
# 2 Second scoring
# Stemming
news_data_corpus = tm_map(news_data_corpus, content_transformer(stemming))
df_news = data.frame(text = sapply(news_data_corpus, as.character),
stringsAsFactors = FALSE)
results_2 = scores.sentiment(df_news$text, senti, negasi)
# 3 Check Inconsistency from lexicon-based approachs
check_data = data.frame(results_1,
results_2)
View(check_data)
check_data$final = check_data$score + check_data$score.1
# 4 Convert score to sentiment classes
check_data$class = as.factor(ifelse(check_data$final < 0, 'Negative',
ifelse(check_data$final == 0, 'Neutral','Positive')))
save_data = data.frame(news_data,
class = check_data$class)
write.csv(x = save_data,
file = 'file:///D:/Audhi Aprilliant/IPB Works/Statistics Department/My Undergraduate Thesis/You Can Do It, Audhi/4 Datasets/6 Forecasting Data/0 News with Sentiment/8 Kompas  2019-01-01 and 2019-12-30 News with Class.csv',
row.names = FALSE,
quote = FALSE)
# 5 Generating daily percentage of sentiment
news_data_senti = data.frame(news_data,
class = check_data$class)
unique(news_data_senti$class)
date_data = as.character(unique(news_data_senti$date))
number_pos = rep(x = 0,length.out = length(date_data))
number_neg = rep(x = 0,length.out = length(date_data))
# Count number of daily positive sentiment
for (i in 1:length(date_data)) {
for (j in 1:dim(news_data_senti)[1]) {
if (date_data[i] == news_data_senti$date[j] & news_data_senti$class[j] == 'Positive') {
number_pos[i] = number_pos[i] + 1
}
else {
number_pos[i] = number_pos[i] + 0
}
}
}
# Count number of daily negative sentiment
for (i in 1:length(date_data)) {
for (j in 1:dim(news_data_senti)[1]) {
if (date_data[i] == news_data_senti$date[j] & news_data_senti$class[j] == 'Negative') {
number_neg[i] = number_neg[i] + 1
}
else {
number_neg[i] = number_neg[i] + 0
}
}
}
# Compiled data
news_data_senti_daily = data.frame(date = date_data,
num_pos = number_pos,
num_neg = number_neg)
news_data_senti_daily
write.csv(x = news_data_senti_daily,
file = 'file:///D:/Audhi Aprilliant/IPB Works/Statistics Department/My Undergraduate Thesis/You Can Do It, Audhi/4 Datasets/6 Forecasting Data/1 Daily of Sentiment Percentage/8 Kompas 2019-01-01 and 2019-12-30 Daily Percentage of Sentiment.csv',
row.names = FALSE)
# 6 Stopwords removal
rm_stopword = VCorpus(VectorSource(results_2$text))
# Using edited stopword list
stopwords_id = readLines('D:/Audhi Aprilliant/IPB Works/Statistics Department/My Undergraduate Thesis/You Can Do It, Audhi/5 R-Scripts/Helpers/Data Helper/stopwords-id.txt')
rm_stopword = tm_map(rm_stopword, removeWords, stopwords_id)
# Check the final result
inspect(rm_stopword[[5]])
# Eliminate extra white spaces
rm_stopword = tm_map(rm_stopword, stripWhitespace)
# Save the data
clean_data = data.frame(text = sapply(rm_stopword, as.character),
stringsAsFactors = FALSE)
clean_data = data.frame(date = news_data$date,
title = clean_data,
time = news_data$time)
# Check the final result
inspect(rm_stopword[[5]])
# Save the data
write.csv(x = clean_data,
file = 'file:///D:/Audhi Aprilliant/IPB Works/Statistics Department/My Undergraduate Thesis/You Can Do It, Audhi/4 Datasets/6 Forecasting Data/2 Clean Headline News for TF-IDF/8 Kompas 2019-01-01 and 2019-12-30 Clean Title Stopwords Removal.csv',
row.names = FALSE)
# LOAD DATA
news_data = read.csv(file = 'file:///D:/Audhi Aprilliant/IPB Works/Statistics Department/My Undergraduate Thesis/You Can Do It, Audhi/4 Datasets/2 News Data/Detik Data - Financial News/Clean Title - Financial Detik 2019-01-01 and 2019-12-30.csv',
header = TRUE,
sep = ','
#,stringsAsFactors = FALSE,
#fileEncoding = "latin1"
)
dim(news_data)
news_data_txt = news_data$title
length(news_data_txt)
# WORK WITH CORPUS
news_data_corpus = VCorpus(VectorSource(news_data_txt))
inspect(news_data_corpus[[5]])
# 1 Tokenization - case folding and filtering
news_data_corpus = tm_map(news_data_corpus, content_transformer(tolower))
news_data_corpus = tm_map(news_data_corpus, toSpace, "[[:punct:]]") # punctuation
news_data_corpus = tm_map(news_data_corpus, toSpace, "[[:digit:]]") # numbers
news_data_corpus = tm_map(news_data_corpus, stripWhitespace)        # Eliminate extra white spaces
# 2 First scoring
pos = readLines('D:/Audhi Aprilliant/IPB Works/Statistics Department/My Undergraduate Thesis/You Can Do It, Audhi/5 R-Scripts/Helpers/Data Helper/s-pos.txt')
neg = readLines('D:/Audhi Aprilliant/IPB Works/Statistics Department/My Undergraduate Thesis/You Can Do It, Audhi/5 R-Scripts/Helpers/Data Helper/s-neg.txt')
df_news = data.frame(text = sapply(news_data_corpus, as.character),
stringsAsFactors = FALSE)
# Negation Handling
negasi = scan('D:/Audhi Aprilliant/IPB Works/Statistics Department/My Undergraduate Thesis/You Can Do It, Audhi/5 R-Scripts/Helpers/Data Helper/negatingword.txt',
what = 'character')
senti = read.csv('file:///D:/Audhi Aprilliant/IPB Works/Statistics Department/My Undergraduate Thesis/You Can Do It, Audhi/5 R-Scripts/Helpers/Data Helper/sentiwords_id_compiled.txt',
sep = ':',
header = FALSE) %>%
mutate(words = as.character(V1),
score = as.numeric(V2)) %>%
select(c('words','score'))
booster = read.csv('D:/Audhi Aprilliant/IPB Works/Statistics Department/My Undergraduate Thesis/You Can Do It, Audhi/5 R-Scripts/Helpers/Data Helper/boosterwords_id.txt',
sep = ":",
header = FALSE) %>%
mutate(words = as.character(V1),
score = as.numeric(V2)) %>%
select(c('words','score'))
results_1 = scores.sentiment(df_news$text, senti, negasi)
View(results_1)
# 2 Second scoring
# Stemming
news_data_corpus = tm_map(news_data_corpus, content_transformer(stemming))
df_news = data.frame(text = sapply(news_data_corpus, as.character),
stringsAsFactors = FALSE)
results_2 = scores.sentiment(df_news$text, senti, negasi)
# 3 Check Inconsistency from lexicon-based approachs
check_data = data.frame(results_1,
results_2)
View(check_data)
check_data$final = check_data$score + check_data$score.1
# 4 Convert score to sentiment classes
check_data$class = as.factor(ifelse(check_data$final < 0, 'Negative',
ifelse(check_data$final == 0, 'Neutral','Positive')))
save_data = data.frame(news_data,
class = check_data$class)
write.csv(x = save_data,
file = 'file:///D:/Audhi Aprilliant/IPB Works/Statistics Department/My Undergraduate Thesis/You Can Do It, Audhi/4 Datasets/6 Forecasting Data/0 News with Sentiment/9 Detik  2019-01-01 and 2019-12-30 News with Class.csv',
row.names = FALSE,
quote = FALSE)
# 5 Generating daily percentage of sentiment
news_data_senti = data.frame(news_data,
class = check_data$class)
View(news_data_senti)
unique(news_data_senti$class)
date_data = as.character(unique(news_data_senti$date))
number_pos = rep(x = 0,length.out = length(date_data))
number_neg = rep(x = 0,length.out = length(date_data))
# Count number of daily positive sentiment
for (i in 1:length(date_data)) {
for (j in 1:dim(news_data_senti)[1]) {
if (date_data[i] == news_data_senti$date[j] & news_data_senti$class[j] == 'Positive') {
number_pos[i] = number_pos[i] + 1
}
else {
number_pos[i] = number_pos[i] + 0
}
}
}
number_pos
# Count number of daily negative sentiment
for (i in 1:length(date_data)) {
for (j in 1:dim(news_data_senti)[1]) {
if (date_data[i] == news_data_senti$date[j] & news_data_senti$class[j] == 'Negative') {
number_neg[i] = number_neg[i] + 1
}
else {
number_neg[i] = number_neg[i] + 0
}
}
}
# Compiled data
news_data_senti_daily = data.frame(date = date_data,
num_pos = number_pos,
num_neg = number_neg)
write.csv(x = news_data_senti_daily,
file = 'file:///D:/Audhi Aprilliant/IPB Works/Statistics Department/My Undergraduate Thesis/You Can Do It, Audhi/4 Datasets/6 Forecasting Data/1 Daily of Sentiment Percentage/9 Detik 2019-01-01 and 2019-12-30 Daily Percentage of Sentiment.csv',
row.names = FALSE)
# 6 Stopwords removal
rm_stopword = VCorpus(VectorSource(results_2$text))
# Using edited stopword list
stopwords_id = readLines('D:/Audhi Aprilliant/IPB Works/Statistics Department/My Undergraduate Thesis/You Can Do It, Audhi/5 R-Scripts/Helpers/Data Helper/stopwords-id.txt')
rm_stopword = tm_map(rm_stopword, removeWords, stopwords_id)
# Check the final result
inspect(rm_stopword[[5]])
# Eliminate extra white spaces
rm_stopword = tm_map(rm_stopword, stripWhitespace)
# Check the final result
inspect(rm_stopword[[5]])
# Save the data
clean_data = data.frame(text = sapply(rm_stopword, as.character),
stringsAsFactors = FALSE)
clean_data = data.frame(date = news_data$date,
title = clean_data,
time = news_data$time)
# Save the data
write.csv(x = clean_data,
file = 'file:///D:/Audhi Aprilliant/IPB Works/Statistics Department/My Undergraduate Thesis/You Can Do It, Audhi/4 Datasets/6 Forecasting Data/2 Clean Headline News for TF-IDF/9 Detik 2019-01-01 and 2019-12-30 Clean Title Stopwords Removal.csv',
row.names = FALSE)
# HEADLINE NEWS OKEZONE
okezone_data = read.csv(file = 'file:///D:/Audhi Aprilliant/IPB Works/Statistics Department/My Undergraduate Thesis/You Can Do It, Audhi/4 Datasets/6 Forecasting Data/2 Clean Headline News for TF-IDF/7 Okezone 2019-01-01 and 2019-12-30 Clean Title Stopwords Removal.csv',
header = TRUE,
sep = ',')
str(okezone_data)
# Make it character
okezone_data$date = as.character(okezone_data$date)
okezone_data$text = as.character(okezone_data$text)
# Count daily average of TF-IDF
date_data = as.character(unique(okezone_data$date))
tfidf_okezone = rep(x = 0,
length.out = length(date_data))
for (i in 1:length(date_data)) {
index_date = which(okezone_data$date == date_data[i])
text_data = VCorpus(VectorSource(okezone_data[index_date,'text']))
tdm = TermDocumentMatrix(text_data,
control = list(wordLengths = c(1, Inf)))
weightedtdm = weightTfIdf(tdm)
matrix_tfidf = as.matrix(weightedtdm)
tfidf_okezone[i] = sum(rowSums(matrix_tfidf))/nrow(matrix_tfidf)
}
# HEADLINE NEWS KOMPAS
kompas_data = read.csv(file = 'file:///D:/Audhi Aprilliant/IPB Works/Statistics Department/My Undergraduate Thesis/You Can Do It, Audhi/4 Datasets/6 Forecasting Data/2 Clean Headline News for TF-IDF/8 Kompas 2019-01-01 and 2019-12-30 Clean Title Stopwords Removal.csv',
header = TRUE,
sep = ',')
str(kompas_data)
# Make it character
kompas_data$date = as.character(kompas_data$date)
kompas_data$text = as.character(kompas_data$text)
# Count daily average of TF-IDF
date_data = as.character(unique(kompas_data$date))
tfidf_kompas = rep(x = 0,
length.out = length(date_data))
for (i in 1:length(date_data)) {
index_date = which(kompas_data$date == date_data[i])
text_data = VCorpus(VectorSource(kompas_data[index_date,'text']))
tdm = TermDocumentMatrix(text_data,
control = list(wordLengths = c(1, Inf)))
weightedtdm = weightTfIdf(tdm)
matrix_tfidf = as.matrix(weightedtdm)
tfidf_kompas[i] = sum(rowSums(matrix_tfidf))/nrow(matrix_tfidf)
}
# HEADLINE NEWS DETIK
detik_data = read.csv(file = 'file:///D:/Audhi Aprilliant/IPB Works/Statistics Department/My Undergraduate Thesis/You Can Do It, Audhi/4 Datasets/6 Forecasting Data/2 Clean Headline News for TF-IDF/9 Detik 2019-01-01 and 2019-12-30 Clean Title Stopwords Removal.csv',
header = TRUE,
sep = ',')
str(detik_data)
# Make it character
detik_data$date = as.character(detik_data$date)
detik_data$text = as.character(detik_data$text)
# Count daily average of TF-IDF
date_data = as.character(unique(detik_data$date))
tfidf_detik = rep(x = 0,
length.out = length(date_data))
for (i in 1:length(date_data)) {
index_date = which(detik_data$date == date_data[i])
text_data = VCorpus(VectorSource(detik_data[index_date,'text']))
tdm = TermDocumentMatrix(text_data,
control = list(wordLengths = c(1, Inf)))
weightedtdm = weightTfIdf(tdm)
matrix_tfidf = as.matrix(weightedtdm)
tfidf_detik[i] = sum(rowSums(matrix_tfidf))/nrow(matrix_tfidf)
}
# HEADLINE NEWS DETIK
detik_data = read.csv(file = 'file:///D:/Audhi Aprilliant/IPB Works/Statistics Department/My Undergraduate Thesis/You Can Do It, Audhi/4 Datasets/6 Forecasting Data/2 Clean Headline News for TF-IDF/9 Detik 2019-01-01 and 2019-12-30 Clean Title Stopwords Removal.csv',
header = TRUE,
sep = ',')
str(detik_data)
# Make it character
detik_data$date = as.character(detik_data$date)
detik_data$text = as.character(detik_data$text)
# Count daily average of TF-IDF
date_data = as.character(unique(detik_data$date))
tfidf_detik = rep(x = 0,
length.out = length(date_data))
for (i in 1:length(date_data)) {
index_date = which(detik_data$date == date_data[i])
text_data = VCorpus(VectorSource(detik_data[index_date,'text']))
tdm = TermDocumentMatrix(text_data,
control = list(wordLengths = c(1, Inf)))
weightedtdm = weightTfIdf(tdm)
matrix_tfidf = as.matrix(weightedtdm)
tfidf_detik[i] = sum(rowSums(matrix_tfidf))/nrow(matrix_tfidf)
}
tfidf_detik
# COMBINE EACH PORTAL NEWS
tfidf_news = data.frame(date = date_data,
tfidf_okezone = as.numeric(tfidf_okezone),
tfidf_kompas = as.numeric(tfidf_kompas),
tfidf_detik = as.numeric(tfidf_detik))
View(tfidf_news)
str(tfidf_news)
# SAVE THE DATA
write.csv(x = tfidf_news,
file = 'file:///D:/Audhi Aprilliant/IPB Works/Statistics Department/My Undergraduate Thesis/You Can Do It, Audhi/4 Datasets/6 Forecasting Data/4 Daily TF-IDF/3 Daily TF-IDF 2019-01-01 and 2019-12-30.csv',
row.names = FALSE)
# THIRD
# Daily Total Sentiment
okezone_senti_3 = read.csv(file = 'file:///D:/Audhi Aprilliant/IPB Works/Statistics Department/My Undergraduate Thesis/You Can Do It, Audhi/4 Datasets/6 Forecasting Data/1 Daily of Sentiment Percentage/7 Okezone 2019-01-01 and 2019-12-30 Daily Percentage of Sentiment.csv',
header = TRUE,
sep = ',')
kompas_senti_3 = read.csv(file = 'file:///D:/Audhi Aprilliant/IPB Works/Statistics Department/My Undergraduate Thesis/You Can Do It, Audhi/4 Datasets/6 Forecasting Data/1 Daily of Sentiment Percentage/8 Kompas 2019-01-01 and 2019-12-30 Daily Percentage of Sentiment.csv',
header = TRUE,
sep = ',')
detik_senti_3 = read.csv(file = 'file:///D:/Audhi Aprilliant/IPB Works/Statistics Department/My Undergraduate Thesis/You Can Do It, Audhi/4 Datasets/6 Forecasting Data/1 Daily of Sentiment Percentage/9 Detik 2019-01-01 and 2019-12-30 Daily Percentage of Sentiment.csv',
header = TRUE,
sep = ',')
full_senti_3 = data.frame(date = okezone_senti_3$date,
pos_okezone = okezone_senti_3$num_pos,
neg_okezone = okezone_senti_3$num_neg,
pos_kompas = kompas_senti_3$num_pos,
neg_kompas = kompas_senti_3$num_neg,
pos_detik = detik_senti_3$num_pos,
neg_detik = detik_senti_3$num_neg)
# Daily Volume News
volume_data_3 = read.csv(file = 'file:///D:/Audhi Aprilliant/IPB Works/Statistics Department/My Undergraduate Thesis/You Can Do It, Audhi/4 Datasets/6 Forecasting Data/3 Daily of Volume News/3 Daily Volume News 2019-01-01 and 2019-12-30.csv',
header = TRUE,
sep = ',')
colnames(volume_data_3)
# Daily Average of TF-IDF
tfidf_data_3 = read.csv(file = 'file:///D:/Audhi Aprilliant/IPB Works/Statistics Department/My Undergraduate Thesis/You Can Do It, Audhi/4 Datasets/6 Forecasting Data/4 Daily TF-IDF/3 Daily TF-IDF 2019-01-01 and 2019-12-30.csv',
header = TRUE,
sep = ',')
colnames(tfidf_data_3)
# Combined
full_data_3 = data.frame(full_senti_3,
volume_data_3[,-1],
tfidf_data_3[,-1])
colnames(full_data_3)
View(full_data_3)
# SAVE FINAL DATA
write.csv(x = full_data_3,
file = 'file:///D:/Audhi Aprilliant/IPB Works/Statistics Department/My Undergraduate Thesis/You Can Do It, Audhi/4 Datasets/2 Final Data - Alhamdulillah.csv',
row.names = FALSE)
# FULL DATA FROM FEATURES EXTRACTION
news_data = read.csv(file = 'file:///D:/Audhi Aprilliant/IPB Works/Statistics Department/My Undergraduate Thesis/You Can Do It, Audhi/4 Datasets/3 Full Final Data - Alhamdulillah.csv',
header = TRUE,
sep = ',')
View(news_data)
colnames(news_data)
str(news_data)
# DAILY PERCENTAGE OF SENTIMENT
news_data$pos_okezone = news_data$pos_okezone / news_data$vn_okezone
news_data$pos_kompas = news_data$pos_kompas / news_data$vn_kompas
news_data$pos_detik = news_data$pos_detik / news_data$vn_detik
news_data$neg_okezone = news_data$neg_okezone / news_data$vn_okezone
news_data$neg_kompas = news_data$neg_kompas / news_data$vn_kompas
news_data$neg_detik = news_data$neg_detik / news_data$vn_detik
View(news_data)
library(e1071)
